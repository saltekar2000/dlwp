{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION\n",
    "\n",
    "In this Jupyter notebook we build a model to classify movie reviews using the IMDB dataset in Keras.  The first part of the notebook follows chapter 3.4 of the Deep Learning With Python (DLWP) book and replicates the baseline model.  The later parts of the notebook contain experiments intended to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules\n",
    "import itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# shortcuts to submodules\n",
    "imdb = tf.keras.datasets.imdb\n",
    "models = tf.keras.models\n",
    "layers = tf.keras.layers\n",
    "optimizers = tf.keras.optimizers\n",
    "losses = tf.keras.losses\n",
    "metrics = tf.keras.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset of reviews and their labels\n",
    "# limit review text to top 10000 commonly occuring words\n",
    "NUM_WORDS = 10000\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = NUM_WORDS)\n",
    "min_index = min([min(s) for s in train_data])\n",
    "max_index = max([max(s) for s in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary of train_data\n",
      "---------------------\n",
      "type: <class 'numpy.ndarray'>\n",
      "shape: (25000,)\n",
      "type of train_data[0]: <class 'list'>\n",
      "length of train_data[:5]: [218, 189, 141, 550, 147]\n",
      "type of train_data[0][0]: <class 'int'>\n",
      "train_data[0][:10]: [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]\n",
      "minimum entry in train_data: 1\n",
      "maximum entry in train_data: 9999\n",
      "\n",
      "summary of train_labels\n",
      "-----------------------\n",
      "type: <class 'numpy.ndarray'>\n",
      "shape: (25000,)\n",
      "type of train_labels[0]: <class 'numpy.int64'>\n",
      "train_labels[:10]: [1 0 0 1 0 0 1 0 1 0]\n",
      "number of negative reviews: 12500\n",
      "number of positive reviews: 12500\n",
      "\n",
      "summary of test_data\n",
      "--------------------\n",
      "shape: (25000,)\n",
      "\n",
      "summary of test_labels\n",
      "----------------------\n",
      "shape: (25000,)\n",
      "number of negative reviews: 12500\n",
      "number of positive reviews: 12500\n"
     ]
    }
   ],
   "source": [
    "print('summary of train_data')\n",
    "print('---------------------')\n",
    "print('type: ' + str(type(train_data)))\n",
    "print('shape: ' + str(train_data.shape))\n",
    "print('type of train_data[0]: ' + str(type(train_data[0])))\n",
    "print('length of train_data[:5]: ' + str([len(train_data[i]) for i in range(5)]))\n",
    "print('type of train_data[0][0]: ' + str(type(train_data[0][0])))\n",
    "print('train_data[0][:10]: ' + str(train_data[0][:10]))\n",
    "print('minimum entry in train_data: ' + str(min_index))\n",
    "print('maximum entry in train_data: ' + str(max_index))\n",
    "print()\n",
    "print('summary of train_labels')\n",
    "print('-----------------------')\n",
    "print('type: ' + str(type(train_labels)))\n",
    "print('shape: ' + str(train_labels.shape))\n",
    "print('type of train_labels[0]: ' + str(type(train_labels[0])))\n",
    "print('train_labels[:10]: ' + str(train_labels[:10]))\n",
    "print('number of negative reviews: ' + str(np.sum(train_labels == 0)))\n",
    "print('number of positive reviews: ' + str(np.sum(train_labels == 1)))\n",
    "print()\n",
    "print('summary of test_data')\n",
    "print('--------------------')\n",
    "print('shape: ' + str(test_data.shape))\n",
    "print()\n",
    "print('summary of test_labels')\n",
    "print('----------------------')\n",
    "print('shape: ' + str(test_labels.shape))\n",
    "print('number of negative reviews: ' + str(np.sum(test_labels == 0)))\n",
    "print('number of positive reviews: ' + str(np.sum(test_labels == 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word to index mapping and create reverse mapping from index to word\n",
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = { value:key for key,value in word_index.items() }\n",
    "\n",
    "# helper function to decode index encoded reviews\n",
    "def decode_review( encoded_review ) : \n",
    "    return ' '.join([reverse_word_index.get(i-3, '?') for i in encoded_review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "number of entries word to index mapping dictionary: 88584\n",
      "number of entries index to word mapping dictionary: 88584\n",
      "low index values are common words: ['the', 'and', 'a', 'of', 'to', 'is', 'br', 'in', 'it']\n",
      "\n",
      "example of a positive review:\n",
      "-----------------------------\n",
      "? lavish production values and solid performances in this straightforward adaption of jane ? satirical classic about the marriage game within and between the classes in ? 18th century england northam and paltrow are a ? mixture as friends who must pass through ? and lies to discover that they love each other good humor is a ? virtue which goes a long way towards explaining the ? of the aged source material which has been toned down a bit in its harsh ? i liked the look of the film and how shots were set up and i thought it didn't rely too much on ? of head shots like most other films of the 80s and 90s do very good results\n",
      "\n",
      "example of a negative review:\n",
      "-----------------------------\n",
      "? this has to be one of the worst films of the 1990s when my friends i were watching this film being the target audience it was aimed at we just sat watched the first half an hour with our jaws touching the floor at how bad it really was the rest of the time everyone else in the theatre just started talking to each other leaving or generally crying into their popcorn that they actually paid money they had ? working to watch this feeble excuse for a film it must have looked like a great idea on paper but on film it looks like no one in the film has a clue what is going on crap acting crap costumes i can't get across how ? this is to watch save yourself an hour a bit of your life\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print('number of entries word to index mapping dictionary: ' + str(len(word_index)))\n",
    "print('number of entries index to word mapping dictionary: ' + str(len(reverse_word_index)))\n",
    "print('low index values are common words: ' + str([reverse_word_index[i] for i in range(1,10)]))\n",
    "print()\n",
    "print('example of a positive review:')\n",
    "print('-----------------------------')\n",
    "positive_index = 6\n",
    "print(decode_review(train_data[positive_index]))\n",
    "print()\n",
    "print('example of a negative review:')\n",
    "print('-----------------------------')\n",
    "negative_index = 2\n",
    "print(decode_review(train_data[negative_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "Encode each review as a 10000-dimensional vector.  Each component of this vector is 0 or 1 indicating the absence or presence of the corresponding word in the review.  An alternate encoding counts the number of times the corresponding word is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary encoding of reviews - 1 if word is present\n",
    "def vectorize_sequences1( sequences, dimension = NUM_WORDS ):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i,sequence] = 1\n",
    "    return results\n",
    "\n",
    "# \"histogram\" encoding of reviews - counts number of times word is present\n",
    "def vectorize_sequences2( sequences, dimension = NUM_WORDS ):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i,sequence] += 1\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences1(train_data)\n",
    "x_test = vectorize_sequences1(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize labels -- don't know why this is necessary (int64-->float32?)\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model from the book\n",
    "def build_model1():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(16, activation='relu', input_shape = (NUM_WORDS,)))\n",
    "    model.add(layers.Dense(16, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0827 09:47:08.396245 4572341696 deprecation.py:506] From //anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0827 09:47:08.578969 4572341696 deprecation.py:323] From //anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model = build_model1()\n",
    "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set aside a validation set\n",
    "VALIDATION_SIZE = 10000\n",
    "x_val = x_train[:VALIDATION_SIZE]\n",
    "partial_x_train = x_train[VALIDATION_SIZE:]\n",
    "y_val = y_train[:VALIDATION_SIZE]\n",
    "partial_y_train = y_train[VALIDATION_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 4s 278us/sample - loss: 0.4985 - acc: 0.7837 - val_loss: 0.3654 - val_acc: 0.8710\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 1s 94us/sample - loss: 0.2879 - acc: 0.9055 - val_loss: 0.2934 - val_acc: 0.8879\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 1s 88us/sample - loss: 0.2150 - acc: 0.9243 - val_loss: 0.2748 - val_acc: 0.8925\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 1s 88us/sample - loss: 0.1708 - acc: 0.9431 - val_loss: 0.2760 - val_acc: 0.8898\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 1s 89us/sample - loss: 0.1354 - acc: 0.9559 - val_loss: 0.2931 - val_acc: 0.8838\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 1s 88us/sample - loss: 0.1154 - acc: 0.9631 - val_loss: 0.2984 - val_acc: 0.8856\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 1s 87us/sample - loss: 0.0965 - acc: 0.9685 - val_loss: 0.3188 - val_acc: 0.8812\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 1s 88us/sample - loss: 0.0780 - acc: 0.9783 - val_loss: 0.3371 - val_acc: 0.8805\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 1s 87us/sample - loss: 0.0657 - acc: 0.9825 - val_loss: 0.3678 - val_acc: 0.8819\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 1s 86us/sample - loss: 0.0551 - acc: 0.9856 - val_loss: 0.3867 - val_acc: 0.8768\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 1s 86us/sample - loss: 0.0409 - acc: 0.9911 - val_loss: 0.4994 - val_acc: 0.8553\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 1s 86us/sample - loss: 0.0369 - acc: 0.9909 - val_loss: 0.4455 - val_acc: 0.8727\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 1s 86us/sample - loss: 0.0289 - acc: 0.9937 - val_loss: 0.4771 - val_acc: 0.8723\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 1s 86us/sample - loss: 0.0265 - acc: 0.9938 - val_loss: 0.5081 - val_acc: 0.8697\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 1s 87us/sample - loss: 0.0203 - acc: 0.9965 - val_loss: 0.5409 - val_acc: 0.8693\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 1s 86us/sample - loss: 0.0159 - acc: 0.9978 - val_loss: 0.5756 - val_acc: 0.8671\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 1s 86us/sample - loss: 0.0118 - acc: 0.9989 - val_loss: 0.6060 - val_acc: 0.8646\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 1s 86us/sample - loss: 0.0142 - acc: 0.9971 - val_loss: 0.6389 - val_acc: 0.8648\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 1s 86us/sample - loss: 0.0058 - acc: 0.9998 - val_loss: 0.6853 - val_acc: 0.8654\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 1s 86us/sample - loss: 0.0096 - acc: 0.9979 - val_loss: 0.7082 - val_acc: 0.8645\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train, partial_y_train, epochs = 20, \n",
    "                    batch_size = 512, validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a histogram of frequency counts of each index\n",
    "flatlist = list(itertools.chain.from_iterable(train_data))  # concatinate reviews\n",
    "freq = np.bincount(flatlist)/len(flatlist)  # count occurance of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of bigrams from flat list\n",
    "bigramlist = list(zip(flatlist[:-1],flatlist[1:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
